# Warning

This is a heavily modified version of the original Transformer-XL repository ([](https://github.com/kimiyoung/transformer-xl)).

This version is aimed at using the network design for time series instead of text embeddings. It is targetting PyTorch; TF code has been removed.

# Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context

This repository contains the code in both **PyTorch** and **TensorFlow** for our paper
>[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](http://arxiv.org/abs/1901.02860)

>Zihang Dai\*, Zhilin Yang\*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov (*: equal contribution)

>Preprint 2018

